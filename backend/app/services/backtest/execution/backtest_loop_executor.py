"""
å›æµ‹å¾ªç¯æ‰§è¡Œæ¨¡å—
è´Ÿè´£æ ¸å¿ƒå›æµ‹å¾ªç¯çš„æ‰§è¡Œ
"""

from typing import Any, Dict, List, Optional, Tuple
import pandas as pd
import numpy as np
import time
from datetime import datetime
from loguru import logger

from ..core.base_strategy import BaseStrategy
from ..core.portfolio_manager import PortfolioManager
from ..models import SignalType, TradingSignal
from app.core.error_handler import ErrorSeverity, TaskError
# å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
# from .backtest_progress_monitor import backtest_progress_monitor


class BacktestLoopExecutor:
    """å›æµ‹å¾ªç¯æ‰§è¡Œå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å›æµ‹å¾ªç¯æ‰§è¡Œå™¨"""
        self.enable_performance_profiling = False
        self.performance_profiler = None

    async def execute_backtest_loop(
        self,
        strategy: BaseStrategy,
        portfolio_manager: PortfolioManager,
        stock_data: Dict[str, pd.DataFrame],
        trading_dates: List[datetime],
        strategy_config: Optional[Dict[str, Any]] = None,
        task_id: str = None,
        backtest_id: str = None,
        precomputed_signals: Optional[Dict[Tuple[str, datetime], Any]] = None,
        aligned_arrays: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå›æµ‹ä¸»å¾ªç¯"""
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
        from .backtest_progress_monitor import backtest_progress_monitor
        
        total_signals = 0
        executed_trades = 0

        # æ€§èƒ½ç»Ÿè®¡ï¼šä¿¡å·ç”Ÿæˆæ—¶é—´
        signal_generation_times = []
        trade_execution_times = []

        # è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        def _is_task_running(status) -> bool:
            if status is None:
                return False
            # æ”¯æŒå­—ç¬¦ä¸²æˆ–Enum
            try:
                return (
                    status == TaskStatus.RUNNING or status == TaskStatus.RUNNING.value
                )
            except Exception:
                return status == TaskStatus.RUNNING.value

        def check_task_status():
            """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ä»ç„¶å­˜åœ¨ä¸”å¤„äºè¿è¡ŒçŠ¶æ€"""
            if not task_id:
                return True
            try:
                from app.core.database import SessionLocal
                from app.models.task_models import TaskStatus
                from app.repositories.task_repository import TaskRepository

                session = SessionLocal()
                try:
                    task_repo = TaskRepository(session)
                    task = task_repo.get_task_by_id(task_id)
                    if not task:
                        logger.warning(f"ä»»åŠ¡ä¸å­˜åœ¨ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ: {task_id}")
                        return False
                    if not _is_task_running(task.status):
                        logger.warning(f"ä»»åŠ¡çŠ¶æ€ä¸º {task.status}ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ: {task_id}")
                        return False
                    return True
                finally:
                    session.close()
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
                return True  # æ£€æŸ¥å¤±è´¥æ—¶ç»§ç»­æ‰§è¡Œï¼Œé¿å…å› æ£€æŸ¥é”™è¯¯è€Œä¸­æ–­

        # ========== PERFä¼˜åŒ–ï¼šæ‰¹é‡æ”¶é›†æ•°æ®åº“æ“ä½œï¼Œå¾ªç¯ç»“æŸåä¸€æ¬¡æ€§å†™å…¥ ==========
        # é¿å…åœ¨730å¤©å¾ªç¯å†…æ¯å¤©éƒ½åšæ•°æ®åº“æ“ä½œï¼ˆåŸæ¥æ˜¯72ç§’çš„ä¸»è¦ç“¶é¢ˆï¼‰
        _batch_signals_data: List[dict] = []  # æ”¶é›†æ‰€æœ‰ä¿¡å·è®°å½•
        _batch_executed_signals: List[dict] = []  # æ”¶é›†å·²æ‰§è¡Œçš„ä¿¡å·
        _batch_unexecuted_signals: List[dict] = []  # æ”¶é›†æœªæ‰§è¡Œçš„ä¿¡å·
        _current_backtest_id: str | None = None  # ç¼“å­˜ backtest_id
        _BATCH_FLUSH_THRESHOLD = 1000  # æµå¼å†™å…¥é˜ˆå€¼

        # æµå¼å†™å…¥è¾…åŠ©å‡½æ•°ï¼šå½“ç§¯ç´¯è¶³å¤Ÿæ•°æ®æ—¶å†™å…¥æ•°æ®åº“
        async def _flush_batch_to_db(
            signals_data: List[dict],
            executed_signals: List[dict],
            unexecuted_signals: List[dict],
            backtest_id: str | None,
            clear_after: bool = True,
        ) -> None:
            """æµå¼å†™å…¥æ‰¹é‡æ•°æ®åˆ°æ•°æ®åº“"""
            if not task_id:
                return
            total_count = len(signals_data) + len(executed_signals) + len(unexecuted_signals)
            if total_count == 0:
                return

            logger.info(f"ğŸ”„ æµå¼å†™å…¥æ•°æ®åº“: ä¿¡å·={len(signals_data)}, å·²æ‰§è¡Œ={len(executed_signals)}, æœªæ‰§è¡Œ={len(unexecuted_signals)}")

            try:
                from app.core.database import get_async_session_context
                from app.repositories.backtest_detailed_repository import (
                    BacktestDetailedRepository,
                )

                async with get_async_session_context() as session:
                    try:
                        repository = BacktestDetailedRepository(session)

                        # 1. æ‰¹é‡ä¿å­˜æ‰€æœ‰ä¿¡å·è®°å½•
                        if signals_data:
                            await repository.batch_save_signal_records(
                                task_id=task_id,
                                backtest_id=backtest_id,
                                signals_data=list(signals_data),  # å¤åˆ¶åˆ—è¡¨é¿å…æ¸…ç©ºåé—®é¢˜
                            )

                        # 2. æ‰¹é‡æ›´æ–°æœªæ‰§è¡Œä¿¡å·çš„åŸå› 
                        if unexecuted_signals:
                            signal_reasons = [
                                (
                                    sig["stock_code"],
                                    sig["timestamp"],
                                    sig["signal_type"],
                                    sig["execution_reason"]
                                )
                                for sig in unexecuted_signals
                            ]
                            await repository.batch_update_signal_execution_reasons(
                                task_id=task_id,
                                signal_reasons=signal_reasons
                            )

                        # 3. æ‰¹é‡æ ‡è®°å·²æ‰§è¡Œçš„ä¿¡å·
                        if executed_signals:
                            signal_keys = [
                                (
                                    sig["stock_code"],
                                    sig["timestamp"],
                                    sig["signal_type"]
                                )
                                for sig in executed_signals
                            ]
                            await repository.batch_mark_signals_as_executed(
                                task_id=task_id,
                                signal_keys=signal_keys
                            )

                        await session.commit()
                        logger.info(f"âœ… æµå¼å†™å…¥å®Œæˆ: {total_count} æ¡è®°å½•")

                    except Exception as e:
                        await session.rollback()
                        logger.warning(f"æµå¼å†™å…¥æ•°æ®åº“å¤±è´¥: {e}")
            except Exception as e:
                logger.warning(f"æµå¼å†™å…¥æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        # ========== END PERFä¼˜åŒ– ==========

        for i, current_date in enumerate(trading_dates):
            # PERF/BUGFIX: ç»Ÿä¸€åˆå§‹åŒ–è®¡æ—¶å˜é‡ï¼Œé¿å…æŸäº›åˆ†æ”¯/å¼‚å¸¸è·¯å¾„å¼•ç”¨æœªèµ‹å€¼å¯¼è‡´ UnboundLocalError
            slice_time_total = 0.0
            gen_time_total = 0.0
            gen_time_max = 0.0

            # åœ¨å¾ªç¯å¼€å§‹æ—¶æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼ˆæ¯10ä¸ªäº¤æ˜“æ—¥æ£€æŸ¥ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹æ£€æŸ¥ï¼‰
            if task_id and i % 10 == 0 and i > 0:
                if not check_task_status():
                    logger.info(f"ä»»åŠ¡çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ: {task_id}")
                    raise TaskError(
                        message=f"ä»»åŠ¡ {task_id} å·²è¢«åˆ é™¤æˆ–çŠ¶æ€å·²æ”¹å˜ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ",
                        severity=ErrorSeverity.LOW,
                    )
            try:
                # è·å–å½“å‰ä»·æ ¼ï¼ˆPhase3ï¼šä½¿ç”¨å‘é‡åŒ–ä¼˜åŒ–ï¼‰
                current_prices: Dict[str, float] = {}

                if aligned_arrays is not None:
                    # Phase 3 ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–ä»·æ ¼æŸ¥æ‰¾
                    from .vectorized_loop import vectorized_price_lookup, get_portfolio_stocks
                    
                    codes = aligned_arrays.get("stock_codes")
                    code_to_i = aligned_arrays.get("code_to_i")
                    close_mat = aligned_arrays.get("close")
                    valid_mat = aligned_arrays.get("valid")
                    sig_mat = aligned_arrays.get("signal")

                    # ä¼˜åŒ– #5ï¼šç¼“å­˜ portfolio stocks setï¼Œé¿å…é‡å¤è°ƒç”¨
                    portfolio_stocks = set(get_portfolio_stocks(portfolio_manager))
                    need_codes = portfolio_stocks.copy()
                    
                    if isinstance(sig_mat, np.ndarray):
                        # ä¼˜åŒ– #5ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œè·å–æœ‰ä¿¡å·çš„è‚¡ç¥¨
                        sig_idx = np.nonzero(sig_mat[:, i])[0]
                        if len(sig_idx) > 0:
                            need_codes.update(codes[j] for j in sig_idx)

                    # BUGFIX: å¦‚æœæ²¡æœ‰é¢„è®¡ç®—ä¿¡å·ä¸”æŒä»“ä¸ºç©ºï¼Œéœ€è¦ä¸ºæ‰€æœ‰è‚¡ç¥¨è·å–ä»·æ ¼
                    # å¦åˆ™æ— æ³•ç”Ÿæˆä¿¡å·ï¼ˆå› ä¸º generate_signals éœ€è¦å½“å‰ä»·æ ¼ï¼‰
                    if not need_codes:
                        need_codes = set(codes)

                    if need_codes:
                        # æ‰¹é‡æŸ¥æ‰¾ä»·æ ¼ï¼ˆå‘é‡åŒ–ï¼‰
                        for c in need_codes:
                            j = code_to_i.get(c) if isinstance(code_to_i, dict) else None
                            if j is not None and bool(valid_mat[j, i]):
                                current_prices[c] = float(close_mat[j, i])

                else:
                    # [ä¼˜åŒ– 1] é¿å… DataFrame æ‹·è´ï¼šä½¿ç”¨ .values å’Œç¼“å­˜çš„ç´¢å¼•
                    for stock_code, data in stock_data.items():
                        try:
                            # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„ date_to_idx æ˜ å°„
                            date_to_idx = data.attrs.get("_date_to_idx")
                            if date_to_idx is not None and current_date in date_to_idx:
                                idx = date_to_idx[current_date]
                                # ä½¿ç”¨ .values ç›´æ¥è®¿é—®åº•å±‚æ•°ç»„
                                current_prices[stock_code] = float(data['close'].values[idx])
                            elif current_date in data.index:
                                # Fallback: ä½¿ç”¨ ilocï¼ˆæ¯” loc å¿«ï¼‰
                                idx = data.index.get_loc(current_date)
                                current_prices[stock_code] = float(data['close'].values[idx])
                        except Exception:
                            pass

                if not current_prices:
                    continue

                # [P2 ä¼˜åŒ–] æ‰¹é‡è®¾ç½®å½“å‰ä»·æ ¼åˆ°æ•°ç»„ï¼Œåç»­çš„ get_portfolio_value ç­‰æ–¹æ³•
                # å¯ä»¥ç›´æ¥ä½¿ç”¨å‘é‡åŒ–è®¡ç®—ï¼Œé¿å…é‡å¤çš„å­—å…¸æŸ¥æ‰¾
                if hasattr(portfolio_manager, 'set_current_prices'):
                    portfolio_manager.set_current_prices(current_prices)

                # ç”Ÿæˆäº¤æ˜“ä¿¡å·ï¼ˆPhase1ï¼šä¼˜å…ˆç”¨ ndarray signal matrixï¼‰
                all_signals: List[TradingSignal] = []

                if aligned_arrays is not None:
                    sig_mat = aligned_arrays.get("signal")
                    codes = aligned_arrays.get("stock_codes")
                    close_mat = aligned_arrays.get("close")
                    valid_mat = aligned_arrays.get("valid")
                    if isinstance(sig_mat, np.ndarray):
                        sig_idx = np.nonzero(sig_mat[:, i])[0]
                        if sig_idx.size > 0:
                            for j in sig_idx.tolist():
                                if not bool(valid_mat[j, i]):
                                    continue
                                st = int(sig_mat[j, i])
                                if st == 1:
                                    stype = SignalType.BUY
                                elif st == -1:
                                    stype = SignalType.SELL
                                else:
                                    continue
                                code = codes[j]
                                price = float(close_mat[j, i])
                                all_signals.append(
                                    TradingSignal(
                                        timestamp=current_date,
                                        stock_code=code,
                                        signal_type=stype,
                                        strength=1.0,
                                        price=price,
                                        reason="[aligned] precomputed",
                                        metadata=None,
                                    )
                                )

                # è‹¥å¯¹é½æ•°ç»„æœªç”Ÿæˆä¿¡å·ï¼Œå†èµ°åŸæœ‰è·¯å¾„ï¼ˆå…¼å®¹å…¶å®ƒç­–ç•¥ï¼‰
                if not all_signals:
                    # ç”Ÿæˆäº¤æ˜“ä¿¡å·ï¼ˆæ”¯æŒå¹¶è¡Œç”Ÿæˆå¤šè‚¡ç¥¨ä¿¡å·ï¼‰
                    all_signals = []

                # æ€§èƒ½ç›‘æ§ï¼šè®°å½•ä¿¡å·ç”Ÿæˆæ—¶é—´
                signal_start_time = (
                    time.perf_counter() if self.enable_performance_profiling else None
                )

                # ï¿½ï¿½åˆ† profilingï¼šæŠŠ"åˆ‡ç‰‡"å’Œ"ç”Ÿæˆä¿¡å·"æ‹†å¼€è®¡æ—¶ï¼ˆå˜é‡å·²åœ¨å¾ªç¯å¼€å¤´åˆå§‹åŒ–ï¼‰

                
                # è¾…åŠ©å‡½æ•°ï¼šå¿«é€ŸæŸ¥æ‰¾é¢„è®¡ç®—ä¿¡å·
                def get_precomputed_signal_fast(stock_code: str, date: datetime):
                    """
                    [ä¼˜åŒ– 1] ä»é¢„è®¡ç®—å­—å…¸ä¸­å¿«é€ŸæŸ¥æ‰¾ä¿¡å·ï¼Œé¿å… DataFrame æ‹·è´
                    
                    ä¼˜åŒ–ç‚¹ï¼š
                    1. ä¼˜å…ˆä½¿ç”¨ aligned_arrays çš„ numpy æ•°ç»„ï¼ˆO(1) æŸ¥æ‰¾ï¼‰
                    2. ä½¿ç”¨ .values ç›´æ¥è®¿é—®åº•å±‚æ•°ç»„ï¼Œé¿å…åˆ›å»º Series å¯¹è±¡
                    3. ç¼“å­˜ date_to_idx æ˜ å°„ï¼Œé¿å…é‡å¤ get_loc() è°ƒç”¨
                    """
                    if precomputed_signals:
                        signal = precomputed_signals.get((stock_code, date))
                        if signal is not None:
                            # å°†ä¿¡å·ç±»å‹è½¬æ¢ä¸º TradingSignal å¯¹è±¡
                            if isinstance(signal, SignalType):
                                # [ä¼˜åŒ– 1] è·å–å½“å‰ä»·æ ¼ - é¿å… DataFrame æ‹·è´
                                current_price = 0.0
                                
                                try:
                                    # æ–¹æ³• 1: ä¼˜å…ˆä½¿ç”¨ aligned_arraysï¼ˆæœ€å¿«ï¼ŒO(1) æŸ¥æ‰¾ï¼‰
                                    if aligned_arrays is not None:
                                        code_to_i = aligned_arrays.get("code_to_i")
                                        close_mat = aligned_arrays.get("close")
                                        dates = aligned_arrays.get("dates")
                                        
                                        if code_to_i is not None and close_mat is not None and dates is not None:
                                            stock_idx = code_to_i.get(stock_code)
                                            if stock_idx is not None:
                                                # [P1 ä¼˜åŒ–] ä½¿ç”¨ O(1) å­—å…¸æŸ¥æ‰¾æ›¿ä»£ O(n) çš„ np.where
                                                date_to_i = aligned_arrays.get("date_to_i")
                                                date_idx = date_to_i.get(date) if date_to_i else None
                                                
                                                if date_idx is not None:
                                                    # ç›´æ¥ä» numpy æ•°ç»„è¯»å–ï¼Œæ—  pandas å¼€é”€
                                                    price_val = close_mat[stock_idx, date_idx]
                                                    if not np.isnan(price_val):
                                                        current_price = float(price_val)
                                    
                                    # æ–¹æ³• 2: å¦‚æœ aligned_arrays ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼˜åŒ–çš„ DataFrame è®¿é—®
                                    if current_price == 0.0:
                                        data = stock_data.get(stock_code)
                                        if data is not None:
                                            # ä½¿ç”¨ç¼“å­˜çš„ date_to_idx æ˜ å°„ï¼ˆé¿å…é‡å¤ get_locï¼‰
                                            date_to_idx = data.attrs.get("_date_to_idx")
                                            if date_to_idx is not None and date in date_to_idx:
                                                idx = date_to_idx[date]
                                                # ä½¿ç”¨ .values ç›´æ¥è®¿é—®åº•å±‚æ•°ç»„ï¼Œé¿å…åˆ›å»º Series
                                                close_values = data['close'].values
                                                current_price = float(close_values[idx])
                                            elif date in data.index:
                                                # Fallback: ä½¿ç”¨ ilocï¼ˆæ¯” loc å¿«ï¼Œä½†ä»ä¼šè§¦å‘ä¸€äº›å¼€é”€ï¼‰
                                                idx = data.index.get_loc(date)
                                                current_price = float(data['close'].values[idx])
                                
                                except Exception as e:
                                    # é™é»˜å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä»·æ ¼ 0.0
                                    pass
                                
                                return [TradingSignal(
                                    signal_type=signal,
                                    stock_code=stock_code,
                                    timestamp=date,
                                    price=current_price,
                                    strength=1.0,
                                    reason=f"Precomputed signal"
                                )]
                            return [signal] if not isinstance(signal, list) else signal
                    return None

                # PERF OPTIMIZATION: ç¦ç”¨per-dayå¹¶è¡Œï¼Œå› ä¸ºä¿¡å·å·²ç»é¢„è®¡ç®—ï¼Œä¸²è¡Œæ›´å¿«
                if False and self.enable_parallel and len(stock_data) > 3:
                    # å¹¶è¡Œç”Ÿæˆå¤šè‚¡ç¥¨ä¿¡å·
                    # PERF: avoid per-day ThreadPoolExecutor creation and avoid per-stock futures.
                    # We batch stocks into coarse tasks to reduce scheduling overhead.

                    # PERF: switch from "per-day submit many tasks" to "persistent workers".
                    # This dramatically reduces thread scheduling overhead when stock_count is large.
                    import threading

                    # Initialize worker context once (first trading day)
                    if not hasattr(self, "_signal_worker_ctx") or self._signal_worker_ctx is None:
                        items = list(stock_data.items())

                        # Greedy balance chunks by estimated per-stock compute cost.
                        # Cost proxy: number of trading days the stock participates (after warmup) with
                        # a small penalty for missing days.
                        scored = []
                        total_days = len(trading_dates) if trading_dates else 0

                        for code, df in items:
                            try:
                                # count how many trading_dates exist in this df
                                # (O(T) per stock; ok for init and much better load balance than len(df))
                                avail = df.index
                                avail_days = 0
                                for _d in trading_dates:
                                    if _d in avail:
                                        avail_days += 1
                                missing_ratio = (
                                    1.0 - (avail_days / total_days)
                                    if total_days > 0
                                    else 0.0
                                )
                                # warmup skip (executor only calls strategy when idx>=20)
                                effective_days = max(0, avail_days - 20)
                                cost = float(effective_days) * (1.0 + 0.10 * missing_ratio)
                                scored.append((cost, code, df))
                            except Exception:
                                scored.append((0.0, code, df))

                        scored.sort(reverse=True)

                        worker_n = max(1, int(self.max_workers or 1))
                        buckets = [([], 0.0) for _ in range(worker_n)]  # ([(code,df)], total_cost)
                        for cost, code, df in scored:
                            # pick bucket with smallest total_cost
                            bi = min(range(worker_n), key=lambda x: buckets[x][1])
                            buckets[bi][0].append((code, df))
                            buckets[bi] = (buckets[bi][0], buckets[bi][1] + float(cost))

                        chunks: List[List[Tuple[str, pd.DataFrame]]] = [b[0] for b in buckets]

                        shared = {"date": None, "error": None}
                        results: List[Tuple[List[TradingSignal], float, float, float]] = [
                            ([], 0.0, 0.0, 0.0) for _ in range(worker_n)
                        ]

                        barrier_start = threading.Barrier(worker_n + 1)
                        barrier_end = threading.Barrier(worker_n + 1)

                        def _worker(idx: int):
                            nonlocal chunks, shared, results
                            while True:
                                try:
                                    barrier_start.wait()
                                except Exception:
                                    return

                                cd = shared.get("date")
                                if cd is None:
                                    # shutdown signal
                                    try:
                                        barrier_end.wait()
                                    except Exception:
                                        pass
                                    return

                                batch_signals: List[TradingSignal] = []
                                slice_sum = 0.0
                                gen_sum = 0.0
                                gen_max = 0.0

                                try:
                                    for stock_code, data in chunks[idx]:
                                        if cd not in data.index:
                                            continue

                                        t0 = time.perf_counter()
                                        idx_map = None
                                        try:
                                            idx_map = data.attrs.get("_date_to_idx")
                                        except Exception:
                                            idx_map = None
                                        current_idx = (
                                            int(idx_map.get(cd))
                                            if isinstance(idx_map, dict) and cd in idx_map
                                            else int(data.index.get_loc(cd))
                                        )
                                        try:
                                            data.attrs["_current_date"] = cd
                                            data.attrs["_current_idx"] = current_idx
                                        except Exception:
                                            pass
                                        slice_dur = time.perf_counter() - t0
                                        slice_sum += float(slice_dur)

                                        if current_idx < 20:
                                            continue

                                        t1 = time.perf_counter()
                                        # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—ä¿¡å·
                                        sigs = get_precomputed_signal_fast(stock_code, cd)
                                        if sigs is None:
                                            # Fallback: è°ƒç”¨ç­–ç•¥ç”Ÿæˆ
                                            sigs = strategy.generate_signals(data, cd)
                                        gen_dur = time.perf_counter() - t1
                                        gen_sum += float(gen_dur)
                                        if gen_dur > gen_max:
                                            gen_max = float(gen_dur)

                                        if sigs:
                                            try:
                                                md = getattr(sigs[0], "metadata", None)
                                                if md is None:
                                                    sigs[0].metadata = {}
                                                    md = sigs[0].metadata
                                                if isinstance(md, dict):
                                                    md["_perf"] = {
                                                        "gen_wall": float(gen_dur),
                                                        "slice_wall": float(slice_dur),
                                                    }
                                            except Exception:
                                                pass

                                        batch_signals.extend(sigs)

                                    results[idx] = (batch_signals, slice_sum, gen_sum, gen_max)
                                except Exception as e:
                                    shared["error"] = e
                                    results[idx] = ([], slice_sum, gen_sum, gen_max)

                                try:
                                    barrier_end.wait()
                                except Exception:
                                    return

                        threads = []
                        for wi in range(worker_n):
                            t = threading.Thread(target=_worker, args=(wi,), daemon=True)
                            t.start()
                            threads.append(t)

                        self._signal_worker_ctx = {
                            "worker_n": worker_n,
                            "shared": shared,
                            "results": results,
                            "barrier_start": barrier_start,
                            "barrier_end": barrier_end,
                            "threads": threads,
                        }

                    ctx = self._signal_worker_ctx

                    sequential_start = (
                        time.perf_counter() if self.enable_performance_profiling else None
                    )

                    gen_time_max = 0.0

                    # Broadcast date to workers and collect
                    ctx["shared"]["date"] = current_date
                    ctx["shared"]["error"] = None

                    try:
                        ctx["barrier_start"].wait()
                        ctx["barrier_end"].wait()
                    except Exception as e:
                        logger.error(f"å¹¶è¡Œç”Ÿæˆä¿¡å·åŒæ­¥å¤±è´¥: {e}")

                    err = ctx["shared"].get("error")
                    if err is not None:
                        raise err

                    for (signals, slice_sum, gen_sum, gen_max) in ctx["results"]:
                        all_signals.extend(signals)
                        slice_time_total += float(slice_sum)
                        gen_time_total += float(gen_sum)
                        if gen_max and gen_max > gen_time_max:
                            gen_time_max = float(gen_max)

                    # è®°å½•å¹¶è¡ŒåŒ–æ•ˆç‡ï¼ˆä¼°ç®—é¡ºåºæ‰§è¡Œæ—¶é—´ï¼‰
                    if self.enable_performance_profiling and sequential_start:
                        parallel_time = time.perf_counter() - sequential_start
                        estimated_sequential_time = parallel_time * len(stock_data) / max(1, self.max_workers)
                        if i == 0:
                            self.performance_profiler.record_parallel_efficiency(
                                operation_name="signal_generation",
                                sequential_time=estimated_sequential_time,
                                parallel_time=parallel_time,
                                worker_count=self.max_workers,
                            )
                else:
                    gen_time_max = 0.0
                    # é¡ºåºç”Ÿæˆä¿¡å·ï¼ˆè‚¡ç¥¨æ•°é‡å°‘æˆ–ç¦ç”¨å¹¶è¡Œï¼‰
                    for stock_code, data in stock_data.items():
                        if current_date in data.index:
                            # è·å–åˆ°å½“å‰æ—¥æœŸçš„å†å²æ•°æ®
                            t0 = time.perf_counter()
                            # same rationale as parallel path: avoid daily slicing copies
                            idx_map = None
                            try:
                                idx_map = data.attrs.get("_date_to_idx")
                            except Exception:
                                idx_map = None
                            current_idx = (
                                int(idx_map.get(current_date))
                                if isinstance(idx_map, dict) and current_date in idx_map
                                else int(data.index.get_loc(current_date))
                                if current_date in data.index
                                else -1
                            )
                            # Provide fast-path hint for strategies (avoid repeated get_loc)
                            try:
                                data.attrs["_current_date"] = current_date
                                data.attrs["_current_idx"] = current_idx
                            except Exception:
                                pass
                            slice_time_total += time.perf_counter() - t0

                            if current_idx >= 20:
                                try:
                                    t1 = time.perf_counter()
                                    # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—ä¿¡å·
                                    signals = get_precomputed_signal_fast(stock_code, current_date)
                                    
                                    # è°ƒè¯•æ—¥å¿—
                                    if current_idx == 20:  # åªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°
                                        logger.info(f"ğŸ” è°ƒè¯•: stock={stock_code}, date={current_date}, precomputed_signals={'æœ‰' if signals else 'æ— '}")
                                    
                                    if signals is None:
                                        # Fallback: è°ƒç”¨ç­–ç•¥ç”Ÿæˆ
                                        signals = strategy.generate_signals(data, current_date)
                                    
                                    # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•ä¿¡å·å†…å®¹
                                    if signals and current_idx == 20:
                                        logger.info(f"ğŸ” ä¿¡å·å†…å®¹: {signals}")
                                    
                                    _dur = time.perf_counter() - t1
                                    gen_time_total += _dur
                                    if _dur > gen_time_max:
                                        gen_time_max = float(_dur)
                                    all_signals.extend(signals)
                                except Exception as e:
                                    logger.warning(f"ç”Ÿæˆä¿¡å·å¤±è´¥ {stock_code}: {e}")
                                    continue

                # è®°å½•ä¿¡å·ç”Ÿæˆæ—¶é—´
                if self.enable_performance_profiling and signal_start_time and self.performance_profiler:
                    signal_duration = time.perf_counter() - signal_start_time
                    signal_generation_times.append(signal_duration)

                    # åŸæœ‰å£å¾„ï¼šæ•´æ®µä¿¡å·ç”Ÿæˆï¼ˆå«åˆ‡ç‰‡ã€è®¡ç®—æŒ‡æ ‡ã€èåˆç­‰ï¼‰
                    self.performance_profiler.record_function_call(
                        "generate_signals", signal_duration
                    )

                    # æ–°å£å¾„ï¼šæ‹†å¼€çœ‹"åˆ‡ç‰‡"ä¸"ç­–ç•¥ä¿¡å·ç”Ÿæˆ"çš„æ¯”ä¾‹
                    # æ³¨æ„ï¼šå¹¶è¡Œæ¨¡å¼ä¸‹ slice_time_total / gen_time_total æ˜¯"å„çº¿ç¨‹è€—æ—¶æ±‚å’Œ"(work)ï¼Œ
                    # ä¸æ˜¯ wall-clockï¼›ç”¨äºåˆ¤æ–­ CPU work æ„æˆï¼Œä½†ä¸èƒ½ç›´æ¥å½“æˆæ•´ä½“è€—æ—¶ç™¾åˆ†æ¯”ã€‚
                    if slice_time_total > 0:
                        self.performance_profiler.record_function_call(
                            "slice_historical_data_work", float(slice_time_total)
                        )
                    if gen_time_total > 0:
                        self.performance_profiler.record_function_call(
                            "generate_signals_core_work", float(gen_time_total)
                        )

                    # é¢å¤–è®°å½• wall-clock å£å¾„ï¼ˆåŒ generate_signalsï¼Œä½†åå­—æ›´æ˜ç¡®ï¼Œä¾¿äºæŠ¥è¡¨é˜…è¯»ï¼‰
                    self.performance_profiler.record_function_call(
                        "generate_signals_wall", signal_duration
                    )

                    # å¹¶è¡Œè·¯å¾„ä¸‹ critical path è¿‘ä¼¼ï¼šå•æ—¥æœ€æ…¢è‚¡ç¥¨çš„ generate_signals wall
                    if gen_time_max > 0:
                        self.performance_profiler.record_function_call(
                            "generate_signals_core_wall_max", float(gen_time_max)
                        )

                        # çº¿ç¨‹/è°ƒåº¦å¼€é”€ï¼ˆç²—ç•¥ï¼‰ï¼šæ•´æ®µ wall - å•æ—¥æœ€æ…¢å•è‚¡ wall
                        overhead = float(signal_duration) - float(gen_time_max)
                        if overhead > 0:
                            self.performance_profiler.record_function_call(
                                "signal_generation_overhead_wall", overhead
                            )

                    # If StrategyPortfolio attached per-strategy timings, record them once per day.
                    try:
                        perf_sig = None
                        for _s in all_signals:
                            md = getattr(_s, "metadata", None) or {}
                            if isinstance(md, dict) and "portfolio_perf" in md:
                                perf_sig = _s
                                break
                        if perf_sig is not None:
                            md = perf_sig.metadata or {}
                            pp = md.get("portfolio_perf") if isinstance(md, dict) else None
                            if isinstance(pp, dict):
                                sub = pp.get("sub_strategy_times")
                                if isinstance(sub, dict):
                                    for k, v in sub.items():
                                        self.performance_profiler.record_function_call(
                                            f"portfolio_substrategy__{k}", float(v)
                                        )
                                it = pp.get("integrate_time")
                                if it is not None:
                                    self.performance_profiler.record_function_call(
                                        "portfolio_integrate", float(it)
                                    )
                    except Exception:
                        pass

                total_signals += len(all_signals)

                # PERFä¼˜åŒ–ï¼šæ”¶é›†ä¿¡å·è®°å½•åˆ°å†…å­˜ï¼Œå¾ªç¯ç»“æŸåæ‰¹é‡å†™å…¥æ•°æ®åº“
                if task_id and all_signals:
                    try:
                        import uuid

                        # ä½¿ç”¨ä¼ å…¥çš„backtest_idæˆ–ç”Ÿæˆä¸€ä¸ªï¼ˆåªç”Ÿæˆä¸€æ¬¡ï¼‰
                        if _current_backtest_id is None:
                            _current_backtest_id = backtest_id or (
                                f"bt_{task_id[:8]}"
                                if task_id
                                else f"bt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                            )

                        # æ”¶é›†ä¿¡å·è®°å½•åˆ°å†…å­˜åˆ—è¡¨ï¼ˆä¸å†æ¯å¤©å†™æ•°æ®åº“ï¼‰
                        for signal in all_signals:
                            signal_data = {
                                "signal_id": f"sig_{uuid.uuid4().hex[:12]}",
                                "stock_code": signal.stock_code,
                                "stock_name": None,
                                "signal_type": signal.signal_type.name,
                                "timestamp": signal.timestamp,
                                "price": signal.price,
                                "strength": signal.strength,
                                "reason": signal.reason,
                                "metadata": signal.metadata,
                                "executed": False,
                            }
                            _batch_signals_data.append(signal_data)
                    except Exception as e:
                        logger.warning(f"ä¿å­˜ä¿¡å·è®°å½•æ—¶å‡ºé”™: {e}")

                # æ‰§è¡Œäº¤æ˜“ä¿¡å·ï¼ˆæ€§èƒ½ç›‘æ§ï¼‰
                trade_start_time = (
                    time.perf_counter() if self.enable_performance_profiling else None
                )
                trades_this_day = 0
                executed_trade_signals = []  # è®°å½•å·²æ‰§è¡Œçš„äº¤æ˜“å¯¹åº”çš„ä¿¡å·
                unexecuted_signals = []  # è®°å½•æœªæ‰§è¡Œçš„ä¿¡å·åŠåŸå› 

                # ===== trade execution mode =====
                trade_mode = None
                topk_limit: int | None = None  # for post-trade sanity checks
                try:
                    trade_mode = (strategy_config or {}).get("trade_mode")
                except Exception:
                    trade_mode = None

                # --- debug aid: log which trade path is used (only when needed) ---
                try:
                    if current_date.strftime("%Y-%m-%d") in ("2023-05-19", "2023-05-22", "2023-05-23"):
                        logger.info(
                            f"[trade_path] date={current_date.strftime('%Y-%m-%d')} trade_mode={trade_mode} "
                            f"signals={len(all_signals)} strategy_config_keys={list((strategy_config or {}).keys())}"
                        )
                except Exception:
                    pass

                if trade_mode == "topk_buffer":
                    # Daily TopK selection + buffer zone + max changes/day
                    k = int((strategy_config or {}).get("topk", 10))
                    topk_limit = k
                    buffer_n = int((strategy_config or {}).get("buffer", 20))
                    max_changes = int((strategy_config or {}).get("max_changes_per_day", 2))
                    trades_limit = max_changes

                    # Build ranking scores from signals (BUY strength positive, SELL negative)
                    scores: Dict[str, float] = {code: 0.0 for code in stock_data.keys()}
                    for sig in all_signals:
                        s = float(sig.strength or 0.0)
                        if sig.signal_type == SignalType.BUY:
                            scores[sig.stock_code] = max(scores.get(sig.stock_code, 0.0), s)
                        elif sig.signal_type == SignalType.SELL:
                            scores[sig.stock_code] = min(scores.get(sig.stock_code, 0.0), -s)

                    # Rebalance according to TopK+buffer rules
                    executed_trade_signals, unexecuted_signals, trades_this_day = self._rebalance_topk_buffer(
                        portfolio_manager=portfolio_manager,
                        current_prices=current_prices,
                        current_date=current_date,
                        scores=scores,
                        topk=k,
                        buffer_n=buffer_n,
                        max_changes=trades_limit,
                        strategy=strategy,
                        debug=bool((strategy_config or {}).get("debug_topk_buffer", False)),
                    )

                    # Debug: show what was executed on key dates / when trades happen
                    try:
                        if trades_this_day > 0 or current_date.strftime("%Y-%m-%d") in ("2023-05-22",):
                            logger.info(
                                f"[trade_exec][topk_buffer] date={current_date.strftime('%Y-%m-%d')} trades_this_day={trades_this_day} "
                                f"executed={len(executed_trade_signals)} unexecuted={len(unexecuted_signals)} holdings_after={len(portfolio_manager.positions)}"
                            )
                    except Exception:
                        pass

                else:
                    # P1ä¼˜åŒ–ï¼šæ¯æ—¥ç¼“å­˜ portfolio_value å’Œ positionsï¼Œé¿å…å¾ªç¯å†…é‡å¤è®¡ç®—
                    daily_portfolio_value = portfolio_manager.get_portfolio_value(current_prices)
                    daily_positions = portfolio_manager.positions

                    for signal in all_signals:
                        # éªŒè¯ä¿¡å·
                        is_valid, validation_reason = strategy.validate_signal(
                            signal,
                            daily_portfolio_value,
                            daily_positions,
                        )

                        if not is_valid:
                            # éªŒè¯å¤±è´¥ï¼Œè®°å½•æœªæ‰§è¡ŒåŸå› 
                            unexecuted_signals.append(
                                {
                                    "stock_code": signal.stock_code,
                                    "timestamp": signal.timestamp,
                                    "signal_type": signal.signal_type.name,
                                    "execution_reason": validation_reason or "ä¿¡å·éªŒè¯å¤±è´¥",
                                }
                            )
                            continue

                        # éªŒè¯é€šè¿‡ï¼Œå°è¯•æ‰§è¡Œ
                        trade_exec_start = (
                            time.perf_counter()
                            if self.enable_performance_profiling
                            else None
                        )
                        trade, failure_reason = portfolio_manager.execute_signal(
                            signal, current_prices
                        )
                        if self.enable_performance_profiling and trade_exec_start:
                            trade_exec_duration = time.perf_counter() - trade_exec_start
                            trade_execution_times.append(trade_exec_duration)
                            self.performance_profiler.record_function_call(
                                "execute_signal", trade_exec_duration
                            )

                        if trade:
                            executed_trades += 1
                            trades_this_day += 1
                            # è®°å½•å·²æ‰§è¡Œçš„ä¿¡å·ï¼Œç”¨äºåç»­æ ‡è®°
                            executed_trade_signals.append(
                                {
                                    "stock_code": signal.stock_code,
                                    "timestamp": signal.timestamp,
                                    "signal_type": signal.signal_type.name,
                                }
                            )
                        else:
                            # æ‰§è¡Œå¤±è´¥ï¼Œè®°å½•æœªæ‰§è¡ŒåŸå› ï¼ˆä» execute_signal ç›´æ¥è·å–ï¼‰
                            unexecuted_signals.append(
                                {
                                    "stock_code": signal.stock_code,
                                    "timestamp": signal.timestamp,
                                    "signal_type": signal.signal_type.name,
                                    "execution_reason": failure_reason or "æ‰§è¡Œå¤±è´¥ï¼ˆæœªçŸ¥åŸå› ï¼‰",
                                }
                            )

                # è®°å½•äº¤æ˜“æ‰§è¡Œæ€»æ—¶é—´
                if self.enable_performance_profiling and trade_start_time:
                    trade_duration = time.perf_counter() - trade_start_time
                    self.performance_profiler.record_function_call(
                        "execute_trades_batch", trade_duration
                    )

                # PERFä¼˜åŒ–ï¼šæ”¶é›†æœªæ‰§è¡Œå’Œå·²æ‰§è¡Œä¿¡å·åˆ°å†…å­˜ï¼Œå¾ªç¯ç»“æŸåæ‰¹é‡å†™å…¥
                if task_id and unexecuted_signals:
                    _batch_unexecuted_signals.extend(unexecuted_signals)

                if task_id and executed_trade_signals:
                    _batch_executed_signals.extend(executed_trade_signals)

                # PERFä¼˜åŒ–Aï¼šæµå¼å¢é‡å†™å…¥ - æ¯ç§¯ç´¯1000æ¡è®°å½•å°±å†™å…¥ä¸€æ¬¡æ•°æ®åº“
                if task_id and (len(_batch_signals_data) + len(_batch_executed_signals) + len(_batch_unexecuted_signals)) >= _BATCH_FLUSH_THRESHOLD:
                    await _flush_batch_to_db(
                        signals_data=_batch_signals_data,
                        executed_signals=_batch_executed_signals,
                        unexecuted_signals=_batch_unexecuted_signals,
                        backtest_id=_current_backtest_id,
                    )
                    # å†™å…¥åæ¸…ç©ºåˆ—è¡¨
                    _batch_signals_data.clear()
                    _batch_executed_signals.clear()
                    _batch_unexecuted_signals.clear()

                # è®°å½•ç»„åˆå¿«ç…§
                portfolio_manager.record_portfolio_snapshot(
                    current_date, current_prices
                )

                # --- Sanity check (debug): topk_buffer must never exceed topk holdings ---
                # è¿™æ¡åªåšå‘Šè­¦ï¼Œä¸æ”¹å˜äº¤æ˜“è¡Œä¸ºï¼Œç”¨äºå®šä½"æŒä»“æ•°ä¸ºä½•ä¼š>topk"ã€‚
                try:
                    tm = None
                    k_limit = None
                    try:
                        tm = (strategy_config or {}).get("trade_mode")
                        k_limit = int((strategy_config or {}).get("topk", 10))
                    except Exception:
                        tm = None
                        k_limit = None

                    if tm == "topk_buffer" and k_limit is not None:
                        current_holdings = list(portfolio_manager.positions.keys())
                        if len(current_holdings) > int(k_limit):
                            logger.error(
                                f"[topk_buffer][sanity] positions_count={len(current_holdings)} > topk={k_limit} "
                                f"date={current_date.strftime('%Y-%m-%d')} holdings={sorted(current_holdings)}"
                            )
                except Exception as e:
                    logger.warning(f"[topk_buffer][sanity] check failed: {e}")

                # æ›´æ–°è¿›åº¦ç›‘æ§ï¼ˆåŒæ—¶æ›´æ–°æ•°æ®åº“ï¼‰
                # æ€§èƒ½ä¼˜åŒ–: é™ä½æ•°æ®åº“æ›´æ–°é¢‘ç‡ï¼Œä»æ¯5å¤©æ”¹ä¸ºæ¯100å¤©ï¼Œå‡å°‘I/Oå¼€é”€
                if task_id and i % 100 == 0:
                    portfolio_value = portfolio_manager.get_portfolio_value(
                        current_prices
                    )
                    logger.debug(
                        f"å‡†å¤‡æ›´æ–°è¿›åº¦: task_id={task_id}, i={i}, total_days={len(trading_dates)}, signals={len(all_signals)}, trades={trades_this_day}, total_signals={total_signals}, total_trades={executed_trades}"
                    )

                    # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”ï¼ˆå›æµ‹æ‰§è¡Œé˜¶æ®µå 30-90%ï¼Œå³60%çš„è¿›åº¦èŒƒå›´ï¼‰
                    execution_progress = (i + 1) / len(trading_dates) * 100
                    overall_progress = 30 + (execution_progress / 100) * 60  # 30%åˆ°90%

                    # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡è¿›åº¦ï¼ˆåŒ…å«è¯¦ç»†æ•°æ®ï¼‰
                    try:
                        # æ³¨æ„ï¼šdatetime å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œä¸è¦åœ¨æ­¤é‡å¤å¯¼å…¥
                        # å¦åˆ™ä¼šå¯¼è‡´ "cannot access local variable 'datetime'" é”™è¯¯
                        from app.core.database import SessionLocal
                        from app.models.task_models import TaskStatus
                        from app.repositories.task_repository import TaskRepository

                        session = SessionLocal()
                        try:
                            task_repo = TaskRepository(session)

                            # è¯»å–ç°æœ‰çš„ result æ•°æ®
                            existing_task = task_repo.get_task_by_id(task_id)
                            if not existing_task:
                                logger.warning(f"ä»»åŠ¡ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°è¿›åº¦: {task_id}")
                                # ä»»åŠ¡å·²è¢«åˆ é™¤ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ
                                raise TaskError(
                                    message=f"ä»»åŠ¡ {task_id} å·²è¢«åˆ é™¤ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ",
                                    severity=ErrorSeverity.LOW,
                                )
                            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œå¦‚æœä¸æ˜¯è¿è¡Œä¸­ï¼Œåˆ™åœæ­¢æ‰§è¡Œ
                            elif not _is_task_running(existing_task.status):
                                logger.warning(
                                    f"ä»»åŠ¡çŠ¶æ€ä¸º {existing_task.status}ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ: {task_id}"
                                )
                                raise TaskError(
                                    message=f"ä»»åŠ¡ {task_id} çŠ¶æ€ä¸º {existing_task.status}ï¼Œåœæ­¢å›æµ‹æ‰§è¡Œ",
                                    severity=ErrorSeverity.LOW,
                                )
                            else:
                                result_data = existing_task.result or {}
                                if not isinstance(result_data, dict):
                                    result_data = {}
                                progress_data = result_data.get("progress_data", {})
                                if not isinstance(progress_data, dict):
                                    progress_data = {}

                                # æ›´æ–°è¿›åº¦æ•°æ®
                                progress_data.update(
                                    {
                                        "processed_days": i + 1,
                                        "total_days": len(trading_dates),
                                        "current_date": current_date.strftime(
                                            "%Y-%m-%d"
                                        ),
                                        "signals_generated": len(all_signals),
                                        "trades_executed": trades_this_day,
                                        "total_signals": total_signals,
                                        "total_trades": executed_trades,
                                        "portfolio_value": portfolio_value,
                                        "last_updated": datetime.utcnow().isoformat(),
                                    }
                                )

                                result_data["progress_data"] = progress_data

                                # è®°å½•æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
                                logger.info(
                                    f"æ›´æ–°å›æµ‹è¿›åº¦æ•°æ®: task_id={task_id}, processed_days={i+1}, total_days={len(trading_dates)}, signals={total_signals}, trades={executed_trades}, portfolio={portfolio_value:.2f}, progress_data_keys={list(progress_data.keys())}"
                                )

                                task_repo.update_task_status(
                                    task_id=task_id,
                                    status=TaskStatus.RUNNING,
                                    progress=overall_progress,
                                    result=result_data,  # åŒ…å«è¯¦ç»†è¿›åº¦æ•°æ®
                                )

                                # ç¡®ä¿ result å­—æ®µè¢«æ ‡è®°ä¸ºå·²ä¿®æ”¹å¹¶æäº¤
                                session.commit()
                                logger.info(
                                    f"è¿›åº¦æ•°æ®å·²æäº¤åˆ°æ•°æ®åº“: task_id={task_id}, result_data_keys={list(result_data.keys())}, progress_data={progress_data}"
                                )
                        except Exception as inner_error:
                            session.rollback()
                            logger.error(
                                f"æ›´æ–°ä»»åŠ¡è¿›åº¦åˆ°æ•°æ®åº“å¤±è´¥ï¼ˆå†…éƒ¨é”™è¯¯ï¼‰: {inner_error}", exc_info=True
                            )
                            raise
                        finally:
                            session.close()
                    except Exception as db_error:
                        logger.error(f"æ›´æ–°ä»»åŠ¡è¿›åº¦åˆ°æ•°æ®åº“å¤±è´¥: {db_error}", exc_info=True)

                    # æ›´æ–°è¿›ç¨‹å†…çš„è¿›åº¦ç›‘æ§ï¼ˆè™½ç„¶ä¸»è¿›ç¨‹çœ‹ä¸åˆ°ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
                    await backtest_progress_monitor.update_execution_progress(
                        task_id=task_id,
                        processed_days=i + 1,
                        current_date=current_date.strftime("%Y-%m-%d"),
                        signals_generated=len(all_signals),
                        trades_executed=trades_this_day,
                        portfolio_value=portfolio_value,
                    )

                # å®šæœŸè¾“å‡ºè¿›åº¦æ—¥å¿—
                if i % 50 == 0:
                    progress = (i + 1) / len(trading_dates) * 100
                    portfolio_value = portfolio_manager.get_portfolio_value(
                        current_prices
                    )
                    logger.debug(f"å›æµ‹è¿›åº¦: {progress:.1f}%, ç»„åˆä»·å€¼: {portfolio_value:.2f}")

            except Exception as e:
                error_msg = f"å›æµ‹å¾ªç¯é”™è¯¯ï¼Œæ—¥æœŸ: {current_date}, é”™è¯¯: {e}"
                logger.error(error_msg)

                # æ·»åŠ è­¦å‘Šåˆ°è¿›åº¦ç›‘æ§
                if task_id:
                    await backtest_progress_monitor.add_warning(task_id, error_msg)

                continue

        # ========== PERFä¼˜åŒ–ï¼šå¾ªç¯ç»“æŸåå†™å…¥å‰©ä½™æ•°æ® ==========
        # å†™å…¥æµå¼å†™å…¥æœªå¤„ç†å®Œçš„å‰©ä½™æ•°æ®
        if task_id and (len(_batch_signals_data) + len(_batch_executed_signals) + len(_batch_unexecuted_signals)) > 0:
            logger.info(f"ğŸ”„ å†™å…¥å‰©ä½™æ•°æ®: ä¿¡å·={len(_batch_signals_data)}, å·²æ‰§è¡Œ={len(_batch_executed_signals)}, æœªæ‰§è¡Œ={len(_batch_unexecuted_signals)}")
            await _flush_batch_to_db(
                signals_data=_batch_signals_data,
                executed_signals=_batch_executed_signals,
                unexecuted_signals=_batch_unexecuted_signals,
                backtest_id=_current_backtest_id,
            )
        # ========== END PERFä¼˜åŒ– ==========

        # æœ€ç»ˆè¿›åº¦æ›´æ–°
        if task_id:
            final_portfolio_value = portfolio_manager.get_portfolio_value({})
            await backtest_progress_monitor.update_execution_progress(
                task_id=task_id,
                processed_days=len(trading_dates),
                current_date=trading_dates[-1].strftime("%Y-%m-%d")
                if trading_dates
                else None,
                signals_generated=0,
                trades_executed=0,
                portfolio_value=final_portfolio_value,
            )

        # è®°å½•æ€§èƒ½ç»Ÿè®¡åˆ°æ€§èƒ½åˆ†æå™¨
        if self.enable_performance_profiling and self.performance_profiler:
            if signal_generation_times:
                avg_signal_time = sum(signal_generation_times) / len(
                    signal_generation_times
                )
                self.performance_profiler.end_stage(
                    "backtest_execution",
                    {
                        "avg_signal_generation_time": avg_signal_time,
                        "total_signal_generation_calls": len(signal_generation_times),
                    },
                )
            if trade_execution_times:
                avg_trade_time = sum(trade_execution_times) / len(trade_execution_times)
                self.performance_profiler.end_stage(
                    "backtest_execution",
                    {
                        "avg_trade_execution_time": avg_trade_time,
                        "total_trade_execution_calls": len(trade_execution_times),
                    },
                )

        return {
            "total_signals": total_signals,
            "executed_trades": executed_trades,
            "trading_days": len(trading_dates),
        }


