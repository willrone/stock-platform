"""
æ•°æ®åŠ è½½å™¨

è´Ÿè´£åŠ è½½å›æµ‹æ‰€éœ€çš„å†å²æ•°æ®
ä¼˜å…ˆä»Qlibé¢„è®¡ç®—ç»“æœè¯»å–ï¼Œå¦‚æœä¸å¯ç”¨åˆ™fallbackåˆ°Parquetç°åœºè®¡ç®—
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from loguru import logger

from app.core.config import settings
from app.core.error_handler import ErrorContext, ErrorSeverity, TaskError


class DataLoader:
    """æ•°æ®åŠ è½½å™¨"""

    def _is_data_valid(
        self,
        data: pd.DataFrame,
        start_date: datetime,
        end_date: datetime,
        min_rows: int = 30,
        min_coverage_ratio: float = 0.7,
    ) -> bool:
        """ç®€å•çš„æ•°æ®æœ‰æ•ˆæ€§è¿‡æ»¤ï¼šè¡Œæ•°>0 ä¸” è¦†ç›–è¶³å¤Ÿé•¿ï¼Œé¿å…æŠ½æ ·åˆ°ç¼ºå¤±è‚¡ç¥¨å½±å“ç»“æœ"""
        try:
            if data is None or data.empty:
                return False
            if len(data) < min_rows:
                return False
            # coverage ratio: rows / expected business days (rough)
            total_days = (end_date.date() - start_date.date()).days + 1
            expected = max(1, total_days * 5 // 7)
            coverage = len(data) / expected
            return coverage >= min_coverage_ratio
        except Exception:
            return False

    def __init__(
        self, data_dir: str = "data", max_workers: Optional[int] = None
    ):
        # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ˆå¤šè¿›ç¨‹ç¯å¢ƒä¸‹ç›¸å¯¹è·¯å¾„ä¼šå¤±æ•ˆï¼‰
        data_path = Path(data_dir)
        if not data_path.is_absolute():
            # ç›¸å¯¹è·¯å¾„ï¼šä»é¡¹ç›®æ ¹ç›®å½•è§£æ
            # data_loader.py ä½äº backend/app/services/backtest/execution/
            # é¡¹ç›®æ ¹ç›®å½•æ˜¯ willrone/ï¼ˆä¸æ˜¯ willrone/backend/ï¼‰
            # æ•°æ®ç›®å½•æ˜¯ willrone/data/
            project_root = Path(__file__).parent.parent.parent.parent.parent.parent
            data_path = (project_root / data_dir).resolve()
        
        self.data_dir = data_path
        self.max_workers = max_workers  # ç”¨äºå¹¶è¡ŒåŠ è½½æ•°æ®
        self.qlib_data_path = Path(settings.QLIB_DATA_PATH) / "features" / "day"

    def load_stock_data(
        self, stock_code: str, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """
        åŠ è½½è‚¡ç¥¨å†å²æ•°æ®ï¼Œä¼˜å…ˆä»é¢„è®¡ç®—ç»“æœè¯»å–

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            åŒ…å«OHLCVå’Œæ‰€æœ‰æŒ‡æ ‡çš„DataFrame
        """
        try:
            # 1. ä¼˜å…ˆå°è¯•ä»Qlibé¢„è®¡ç®—ç›®å½•åŠ è½½
            precomputed_data = self._load_from_precomputed(
                stock_code, start_date, end_date
            )
            if precomputed_data is not None and not precomputed_data.empty:
                logger.info(
                    f"ä»é¢„è®¡ç®—ç»“æœåŠ è½½: {stock_code}, æŒ‡æ ‡æ•°: {len(precomputed_data.columns)}"
                )
                return precomputed_data

            # 2. Fallbackï¼šä»ParquetåŠ è½½åŸºç¡€æ•°æ®ï¼ˆç°åœºè®¡ç®—æŒ‡æ ‡ï¼‰
            logger.info(f"é¢„è®¡ç®—ç»“æœä¸å¯ç”¨ï¼Œä»ParquetåŠ è½½å¹¶è®¡ç®—: {stock_code}")
            return self._load_from_parquet_and_calculate(
                stock_code, start_date, end_date
            )

        except TaskError:
            raise
        except Exception as e:
            raise TaskError(
                message=f"åŠ è½½è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}",
                severity=ErrorSeverity.HIGH,
                context=ErrorContext(stock_code=stock_code),
                original_exception=e,
            )

    def _load_from_precomputed(
        self, stock_code: str, start_date: datetime, end_date: datetime
    ) -> Optional[pd.DataFrame]:
        """
        ä»Qlibé¢„è®¡ç®—ç›®å½•åŠ è½½æ•°æ®

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            é¢„è®¡ç®—æ•°æ®DataFrameï¼Œå¦‚æœä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            from app.services.data.qlib_format_converter import QlibFormatConverter

            converter = QlibFormatConverter()
            safe_code = stock_code.replace(".", "_")

            # å°è¯•ä»å•è‚¡ç¥¨æ–‡ä»¶åŠ è½½
            stock_file = self.qlib_data_path / f"{safe_code}.parquet"
            if stock_file.exists():
                # Qlib æ–‡ä»¶é€šå¸¸ä»¥ä¸‹åˆ’çº¿å‘½åï¼ˆ000001_SZï¼‰ï¼Œå†…éƒ¨ index level 0 ä¹Ÿå¯èƒ½æ˜¯è¯¥æ ¼å¼ã€‚
                # ä¸ºé¿å… KeyError + å¤§é‡ warningï¼Œç»Ÿä¸€ç”¨ safe_code å»è¯»å–/è¿‡æ»¤ã€‚
                qlib_data = converter.load_qlib_data(
                    stock_file,
                    stock_code=safe_code,
                    start_date=start_date,
                    end_date=end_date,
                )

                if not qlib_data.empty:
                    # è½¬æ¢ä¸ºå›æµ‹éœ€è¦çš„æ ¼å¼ï¼ˆå•è‚¡ç¥¨DataFrameï¼Œç´¢å¼•ä¸ºæ—¥æœŸï¼‰
                    # ä»MultiIndexä¸­æå–å•è‚¡ç¥¨æ•°æ®
                    if isinstance(qlib_data.index, pd.MultiIndex):
                        try:
                            stock_data = qlib_data.xs(
                                safe_code, level=0, drop_level=False
                            )
                            # å°†æ—¥æœŸç´¢å¼•æå–å‡ºæ¥
                            stock_data.index = stock_data.index.get_level_values(1)
                        except KeyError:
                            # å¦‚æœMultiIndexä¸­æ²¡æœ‰è¯¥è‚¡ç¥¨ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
                            if qlib_data.index.nlevels == 2:
                                stock_data = qlib_data.copy()
                                stock_data.index = stock_data.index.get_level_values(1)
                            else:
                                return None
                    else:
                        stock_data = qlib_data.copy()

                    # åˆ—åæ˜ å°„ï¼š$close -> closeç­‰ï¼ˆå›æµ‹ç­–ç•¥æœŸæœ›çš„æ ¼å¼ï¼‰
                    column_mapping = {
                        "$open": "open",
                        "$high": "high",
                        "$low": "low",
                        "$close": "close",
                        "$volume": "volume",
                    }
                    stock_data = stock_data.rename(columns=column_mapping)

                    # ç¡®ä¿å¿…éœ€çš„åˆ—å­˜åœ¨
                    required_cols = ["open", "high", "low", "close", "volume"]
                    if all(col in stock_data.columns for col in required_cols):
                        # æ·»åŠ è‚¡ç¥¨ä»£ç å±æ€§
                        stock_data.attrs["stock_code"] = stock_code
                        stock_data.attrs["from_precomputed"] = True
                        return stock_data

            # å°è¯•ä»åˆå¹¶æ–‡ä»¶åŠ è½½ï¼ˆå¯é€‰ï¼›é»˜è®¤å…³é—­ä»¥é¿å…å¤§é‡ miss å¯¼è‡´ I/O+æ—¥å¿—å¼€é”€ï¼‰
            try:
                use_all = bool(getattr(settings, "QLIB_USE_ALL_STOCKS_FILE", False))
            except Exception:
                use_all = False

            all_stocks_file = self.qlib_data_path / "all_stocks.parquet"
            if use_all and all_stocks_file.exists():
                qlib_data = converter.load_qlib_data(
                    all_stocks_file,
                    stock_code=safe_code,
                    start_date=start_date,
                    end_date=end_date,
                )

                if not qlib_data.empty:
                    # è½¬æ¢ä¸ºå›æµ‹éœ€è¦çš„æ ¼å¼
                    if isinstance(qlib_data.index, pd.MultiIndex):
                        try:
                            stock_data = qlib_data.xs(
                                safe_code, level=0, drop_level=False
                            )
                            stock_data.index = stock_data.index.get_level_values(1)
                        except KeyError:
                            return None
                    else:
                        stock_data = qlib_data.copy()

                    # åˆ—åæ˜ å°„
                    column_mapping = {
                        "$open": "open",
                        "$high": "high",
                        "$low": "low",
                        "$close": "close",
                        "$volume": "volume",
                    }
                    stock_data = stock_data.rename(columns=column_mapping)

                    required_cols = ["open", "high", "low", "close", "volume"]
                    if all(col in stock_data.columns for col in required_cols):
                        stock_data.attrs["stock_code"] = stock_code
                        stock_data.attrs["from_precomputed"] = True
                        return stock_data

            return None

        except Exception as e:
            logger.warning(f"ä»é¢„è®¡ç®—ç»“æœåŠ è½½å¤±è´¥ {stock_code}: {e}")
            return None

    def _load_from_parquet_and_calculate(
        self, stock_code: str, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """
        ä»ParquetåŠ è½½åŸºç¡€æ•°æ®ï¼ˆFallbackæ–¹æ³•ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            åŒ…å«åŸºç¡€OHLCVæ•°æ®çš„DataFrameï¼ˆæŒ‡æ ‡éœ€è¦ç­–ç•¥ä¸­è®¡ç®—ï¼‰
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨
        from app.services.data.stock_data_loader import StockDataLoader

        loader = StockDataLoader(data_root=str(self.data_dir))

        # åŠ è½½æ•°æ®
        data = loader.load_stock_data(
            stock_code, start_date=start_date, end_date=end_date
        )

        if data.empty:
            raise TaskError(
                message=f"æœªæ‰¾åˆ°è‚¡ç¥¨æ•°æ®æ–‡ä»¶: {stock_code}",
                severity=ErrorSeverity.HIGH,
                context=ErrorContext(stock_code=stock_code),
            )

        if len(data) == 0:
            raise TaskError(
                message=f"æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ— æ•°æ®: {stock_code}, {start_date} - {end_date}",
                severity=ErrorSeverity.MEDIUM,
                context=ErrorContext(stock_code=stock_code),
            )

        # éªŒè¯å¿…éœ€çš„åˆ—
        required_columns = ["open", "high", "low", "close", "volume"]
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise TaskError(
                message=f"æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}",
                severity=ErrorSeverity.HIGH,
                context=ErrorContext(stock_code=stock_code),
            )

        # æ·»åŠ è‚¡ç¥¨ä»£ç å±æ€§
        data.attrs["stock_code"] = stock_code
        data.attrs["from_precomputed"] = False

        # [æ€§èƒ½ä¼˜åŒ–] é¢„è®¡ç®—å¸¸ç”¨æŠ€æœ¯æŒ‡æ ‡åˆ—ï¼Œä¾›ç­–ç•¥å¤ç”¨ï¼Œé¿å…æ¯ä¸ªç­–ç•¥é‡å¤ rolling
        try:
            import time as _time
            _t_precomp = _time.perf_counter()
            close = data["close"]

            # å¸¸ç”¨å‡çº¿/æ³¢åŠ¨ï¼ˆå½“å‰éªŒæ”¶ç»„åˆç”¨åˆ°ï¼šMA20/MA50/MA60 + STD20/STD60 + RSI14ï¼‰
            for p in (20, 50, 60):
                col = f"MA{p}"
                if col not in data.columns:
                    data[col] = close.rolling(window=p).mean()

            for p in (20, 60):
                col = f"STD{p}"
                if col not in data.columns:
                    data[col] = close.rolling(window=p).std()

            # RSI14ï¼ˆWilder ç®€åŒ–ç‰ˆï¼Œå’Œç­–ç•¥ fallback ä¿æŒä¸€è‡´å£å¾„ï¼‰
            if "RSI14" not in data.columns:
                delta = close.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                rs = gain / loss
                data["RSI14"] = 100 - (100 / (1 + rs))

            _precomp_ms = (_time.perf_counter() - _t_precomp) * 1000
            if _precomp_ms > 10:
                logger.debug(f"ğŸ“Š DataLoaderé¢„è®¡ç®—æŒ‡æ ‡ [{stock_code}]: {_precomp_ms:.1f}ms, {len(data)}è¡Œ, åˆ—={list(data.columns)}")
        except Exception as e:
            logger.warning(f"é¢„è®¡ç®—å¸¸ç”¨æŒ‡æ ‡å¤±è´¥ {stock_code}: {e}")

        logger.info(
            f"ä»ParquetåŠ è½½è‚¡ç¥¨æ•°æ®: {stock_code}, æ•°æ®é‡: {len(data)}, æ—¥æœŸèŒƒå›´: {data.index[0]} - {data.index[-1]}"
        )
        return data

    def load_multiple_stocks(
        self,
        stock_codes: List[str],
        start_date: datetime,
        end_date: datetime,
        parallel: bool = True,
    ) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½å¤šåªè‚¡ç¥¨æ•°æ®ï¼Œä¼˜å…ˆä»é¢„è®¡ç®—ç»“æœè¯»å–

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            parallel: æ˜¯å¦å¹¶è¡ŒåŠ è½½ï¼ˆé»˜è®¤Trueï¼‰
        """
        stock_data = {}
        failed_stocks = []
        precomputed_count = 0

        if parallel and len(stock_codes) > 1 and self.max_workers:
            # å¹¶è¡ŒåŠ è½½å¤šåªè‚¡ç¥¨æ•°æ®
            max_workers = min(self.max_workers, len(stock_codes))
            logger.info(f"å¹¶è¡ŒåŠ è½½ {len(stock_codes)} åªè‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹")

            def load_single_stock(
                stock_code: str,
            ) -> Tuple[str, Optional[pd.DataFrame], Optional[str], bool]:
                """åŠ è½½å•åªè‚¡ç¥¨æ•°æ®ï¼Œè¿”å› (stock_code, data, error, from_precomputed)"""
                try:
                    data = self.load_stock_data(stock_code, start_date, end_date)
                    from_precomputed = data.attrs.get("from_precomputed", False)
                    return (stock_code, data, None, from_precomputed)
                except Exception as e:
                    error_msg = str(e)
                    logger.error(f"åŠ è½½è‚¡ç¥¨æ•°æ®å¤±è´¥: {stock_code}, é”™è¯¯: {error_msg}")
                    return (stock_code, None, error_msg, False)

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(load_single_stock, code): code
                    for code in stock_codes
                }

                for future in as_completed(futures):
                    stock_code, data, error, from_precomputed = future.result()
                    if data is not None:
                        # data validity filter: avoid missing/too-short coverage polluting universe sampling
                        if self._is_data_valid(data, start_date, end_date):
                            stock_data[stock_code] = data
                            if from_precomputed:
                                precomputed_count += 1
                        else:
                            failed_stocks.append(stock_code)
                    else:
                        failed_stocks.append(stock_code)
        else:
            # é¡ºåºåŠ è½½ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
            for stock_code in stock_codes:
                try:
                    data = self.load_stock_data(stock_code, start_date, end_date)
                    if self._is_data_valid(data, start_date, end_date):
                        stock_data[stock_code] = data
                        if data.attrs.get("from_precomputed", False):
                            precomputed_count += 1
                    else:
                        failed_stocks.append(stock_code)
                except Exception as e:
                    logger.error(f"åŠ è½½è‚¡ç¥¨æ•°æ®å¤±è´¥: {stock_code}, é”™è¯¯: {e}")
                    failed_stocks.append(stock_code)
                    continue

        if precomputed_count > 0:
            logger.info(f"ä»é¢„è®¡ç®—ç»“æœåŠ è½½äº† {precomputed_count}/{len(stock_data)} åªè‚¡ç¥¨çš„æ•°æ®")

        if failed_stocks:
            logger.warning(f"éƒ¨åˆ†è‚¡ç¥¨æ•°æ®åŠ è½½å¤±è´¥: {failed_stocks}")

        if not stock_data:
            raise TaskError(message="æ‰€æœ‰è‚¡ç¥¨æ•°æ®åŠ è½½å¤±è´¥", severity=ErrorSeverity.HIGH)

        return stock_data
